{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Keras package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# just test to see if installation is working...\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining and adding layers to a model:\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# similarly this can be done as follows:\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# before training a model, we need to compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# For a mean squared error regression problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "# defining a model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Activation('softmax'))\n",
    "# defining the optimizer and then compiling it\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "\n",
    "# or just using the default values:\n",
    "# pass optimizer by name: default parameters will be used\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD - Stochastic gradient descent optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.optimizers.SGD at 0xce27dd8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSprop\n",
    "#It is recommended to leave the parameters of this optimizer at their default values (except the learning rate, which can be freely tuned).\n",
    "#This optimizer is usually a good choice for recurrent neural networks.\n",
    "#optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "#http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model (using fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7146 - acc: 0.5220     \n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7010 - acc: 0.5300     \n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6942 - acc: 0.5400     \n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6883 - acc: 0.5560     \n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6834 - acc: 0.5660     \n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6821 - acc: 0.5750     \n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6735 - acc: 0.5910     \n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6720 - acc: 0.5850     \n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6673 - acc: 0.5940     \n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6610 - acc: 0.6230     \n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.3350 - acc: 0.0980     \n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.3095 - acc: 0.1130     \n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.3010 - acc: 0.1270     \n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2949 - acc: 0.1380     \n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2878 - acc: 0.1410     \n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2799 - acc: 0.1550     \n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2750 - acc: 0.1480     \n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2671 - acc: 0.1540     \n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2580 - acc: 0.1670     \n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2504 - acc: 0.1740     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1234d9b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a single-input model with 2 classes (binary classification):\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, epochs=10, batch_size=32)\n",
    "\n",
    "# For a single-input model with 10 classes (categorical classification):\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "# print('labels',labels)\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of some training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.4159 - acc: 0.1250     \n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3995 - acc: 0.1150     \n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3442 - acc: 0.1010     \n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3370 - acc: 0.1010     \n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3191 - acc: 0.1130     \n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3120 - acc: 0.1040     \n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3171 - acc: 0.0910     \n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3060 - acc: 0.1190     \n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3085 - acc: 0.0970     \n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3087 - acc: 0.1070     \n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3001 - acc: 0.1030     \n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3000 - acc: 0.1250     \n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.2981 - acc: 0.1130     \n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3061 - acc: 0.1050     \n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3025 - acc: 0.1060     \n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.2991 - acc: 0.1140     \n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.2926 - acc: 0.1310     \n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.2995 - acc: 0.1180     \n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.2900 - acc: 0.1360     \n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.2937 - acc: 0.1290     \n",
      "100/100 [==============================] - 0s\n",
      "Test score: 2.30481076241\n",
      "Test accuracy: 0.0900000035763\n"
     ]
    }
   ],
   "source": [
    "## Multilayer Perceptron (MLP) for multi-class softmax classification:\n",
    "##---------------------------------------------------------------------\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG (Visual Geometry Group) - like ConvNet\n",
    "Oxford group - paper:\n",
    "designs and tests CNN architectures, conv layers of 3x3 and pooling of 2x2\n",
    "concludes that deeper is better\n",
    "have many variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 3s - loss: 2.3459 - acc: 0.0900     \n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 3s - loss: 2.3583 - acc: 0.1100     \n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 3s - loss: 2.2824 - acc: 0.1300     \n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 3s - loss: 2.2764 - acc: 0.1700     \n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 3s - loss: 2.2469 - acc: 0.1700     \n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 3s - loss: 2.2725 - acc: 0.1200     \n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 3s - loss: 2.2744 - acc: 0.1500     \n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 3s - loss: 2.2778 - acc: 0.0700     \n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 3s - loss: 2.2670 - acc: 0.1700     \n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 3s - loss: 2.2875 - acc: 0.1200     \n",
      "20/20 [==============================] - 0s\n",
      "Test score: 2.32720375061\n",
      "Test accuracy: 0.10000000149\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((100, 100, 100, 3))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "x_test = np.random.random((20, 100, 100, 3))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd,\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.summary(): prints a summary representation of your model.\n",
    "model.get_config(): returns a dictionary containing the configuration of the model. \n",
    "model.get_weights(): returns a list of all weight tensors in the model, as Numpy arrays.\n",
    "model.set_weights(weights): sets the values of the weights of the model, \n",
    "from a list of Numpy arrays. \n",
    "The arrays in the list should have the same shape as those returned by get_weights().\n",
    "model.save_weights(filepath): saves the weights of the model as a HDF5 file.\n",
    "model.load_weights(filepath, by_name=False): loads the weights of the model from a HDF5 file (created by  save_weights). By default, the architecture is expected to be unchanged. To load weights into a different architecture (with some layers in common), use by_name=True to load only those layers with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.get_config()\n",
    "model = Model.from_config(config)\n",
    "# or, for Sequential:\n",
    "model = Sequential.from_config(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
