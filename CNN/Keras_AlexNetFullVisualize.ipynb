{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# WRITTEN BY ALAN LI\n",
    "# NASA AMES LABORATORY FOR ADVANCED SENSING (LAS)\n",
    "# Last edited: Nov 8, 2017\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import loadcoraldata_utils as coralutils\n",
    "import glob, os\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, Convolution2D, MaxPooling2D\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyback(predictions):\n",
    "    return np.argmax(predictions,1)\n",
    "\n",
    "def rescale(dataset, depth):\n",
    "\tdataset_norm = (dataset.astype(np.float32) - depth/2)/(depth/2)\n",
    "\treturn dataset_norm\n",
    "\n",
    "def load_data(Imagepath, Truthpath, truth_key = None):\n",
    "\timg1 = cv2.imread(Imagepath,cv2.IMREAD_UNCHANGED)\n",
    "\timg1_truth = cv2.imread(Truthpath,cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "\tif truth_key is not None:\n",
    "\t\titem_counter = 0\n",
    "\t\tfor item in truth_key:\n",
    "\t\t\timg1_truth[img1_truth == item ] = item_counter  # Sand\n",
    "\t\t\titem_counter+=1\n",
    "\treturn img1, img1_truth.astype(np.uint8)\n",
    "\n",
    "def load_whole_data(img1, img1_truth, image_size, depth=255, offset=0, lines=None, toremove = False):\n",
    "\tcrop_len = int(np.floor(image_size/2))\n",
    "\n",
    "\tif lines is None:\n",
    "\t\tlines = img1.shape[0]-2*crop_len\n",
    "\n",
    "\tif offset + lines + 2*crop_len > img1.shape[0]:\n",
    "\t\tprint(\"Too many lines specified, reverting to maximum possible\")\n",
    "\t\tlines = im1.shape[0] - offset - 2*crop_len\n",
    "\n",
    "\twhole_datasets = []\n",
    "\twhole_labels = []\n",
    "\tfor i in range(offset+crop_len,lines+offset+crop_len):\n",
    "\t\tfor j in range(crop_len, img1.shape[1]-crop_len):\n",
    "\t\t\twhole_datasets.append(img1[i-crop_len:i+crop_len+1, j-crop_len:j+crop_len+1,:])\n",
    "\t\t\twhole_labels.append(img1_truth[i,j])\n",
    "\n",
    "\twhole_datasets = np.asarray(whole_datasets) \n",
    "\twhole_labels = np.asarray(whole_labels).reshape((len(whole_labels),1))\n",
    "\n",
    "\tif toremove is not None:\n",
    "\t\twhole_datasets = np.delete(whole_datasets,toremove,-1)\n",
    "\twhole_dataset = rescale(whole_datasets,depth)\n",
    "\treturn whole_dataset, whole_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on ./models\\weights_epoch00_batch00000500.hdf5\n",
      "Currently on ./models\\weights_epoch00_batch00000550.hdf5\n",
      "Currently on ./models\\weights_epoch00_batch00000600.hdf5\n",
      "Currently on ./models\\weights_epoch00_batch00000650.hdf5\n",
      "Currently on ./models\\weights_epoch00_batch00000700.hdf5\n",
      "Currently on ./models\\weights_epoch00_batch00000750.hdf5\n",
      "Currently on ./models\\weights_epoch00_batch00000800.hdf5\n",
      "Currently on ./models\\weights_epoch00_batch00000850.hdf5\n",
      "Currently on ./models\\weights_epoch00_batch00000900.hdf5\n",
      "Currently on ./models\\weights_epoch00_batch00000950.hdf5\n",
      "Currently on ./models\\weights_epoch00_batch00001000.hdf5\n",
      "Currently on ./models\\weights_epoch00_batch00001050.hdf5\n",
      "Currently on ./models\\weights_epoch00_batch00001100.hdf5\n",
      "Currently on ./models\\weights_epoch00_batch00001150.hdf5\n",
      "Currently on ./models\\weights_epoch00_batch00001200.hdf5\n",
      "Currently on ./models\\weights_epoch01_batch00001250.hdf5\n",
      "Currently on ./models\\weights_epoch01_batch00001300.hdf5\n",
      "Currently on ./models\\weights_epoch01_batch00001350.hdf5\n",
      "Currently on ./models\\weights_epoch01_batch00001400.hdf5\n",
      "Currently on ./models\\weights_epoch01_batch00001450.hdf5\n",
      "Currently on ./models\\weights_epoch01_batch00001500.hdf5\n",
      "Currently on ./models\\weights_epoch01_batch00001550.hdf5\n",
      "Currently on ./models\\weights_epoch01_batch00001600.hdf5\n",
      "Currently on ./models\\weights_epoch01_batch00001650.hdf5\n",
      "Currently on ./models\\weights_epoch01_batch00001700.hdf5\n",
      "Currently on ./models\\weights_epoch01_batch00001750.hdf5\n",
      "Currently on ./models\\weights_epoch01_batch00001800.hdf5\n",
      "Currently on ./models\\weights_epoch01_batch00001850.hdf5\n",
      "Currently on ./models\\weights_epoch01_batch00001900.hdf5\n",
      "Currently on ./models\\weights_epoch01_batch00001950.hdf5\n",
      "13574/ 13574 completed\r"
     ]
    }
   ],
   "source": [
    "offstart = 0\n",
    "num_classes = 4\n",
    "image_size = 25\n",
    "crop_len = int(np.floor(image_size/2))\n",
    "transect1_path = '../Images/Transect 1 Hi-Res.tiff'\n",
    "transect1_truth_path = '../Images/Transect 1 Truth data.tif'\n",
    "Transect1, Transect1_truth = load_data(transect1_path, transect1_truth_path, truth_key=[16,160,198,38])\n",
    "num_lines = Transect1_truth.shape[0] - 2*crop_len\n",
    "#num_lines = 250\n",
    "model = load_model('./models/AlexNetLike.h5')\n",
    "\n",
    "weightfiles = glob.glob('./models/*.hdf5')\n",
    "truth_predict = Transect1_truth[crop_len+offstart:crop_len+offstart+num_lines, crop_len:Transect1_truth.shape[1]-crop_len]\n",
    "f = open('accuracies.txt','a')\n",
    "\n",
    "for i in range(10,40):\n",
    "    whole_predict = []\n",
    "    print(\"Currently on \" + weightfiles[i])\n",
    "    model.load_weights(weightfiles[i])\n",
    "    for offset in range(offstart,offstart+num_lines):\n",
    "        temp_dataset, temp_labelset = load_whole_data(Transect1, Transect1_truth, image_size=25, offset = offset, lines=1, toremove=3)\n",
    "        temp_predict = model.predict_on_batch(temp_dataset)\n",
    "        whole_predict.append(classifyback(temp_predict))\n",
    "        print(str(offset+1) + '/ ' + str(num_lines) +' completed', end='\\r')\n",
    "    whole_predict = np.asarray(whole_predict).astype(np.uint8)\n",
    "    whole_predict_map = np.copy(whole_predict.astype(np.float32))\n",
    "    whole_predict_map[whole_predict_map == 0] = -1\n",
    "    whole_predict_map = ((whole_predict_map+1)*255/4).astype(np.uint8)\n",
    "    whole_predict_map = cv2.applyColorMap(whole_predict_map, cv2.COLORMAP_JET)\n",
    "    \n",
    "    cv2.imwrite(weightfiles[i].split('\\\\')[1].split('.')[0]+'.png',whole_predict_map)\n",
    "    accuracy = 100*(whole_predict == truth_predict).astype(np.float32).sum()/(whole_predict.shape[0]*whole_predict.shape[1])\n",
    "    f.write('{0} \\n'.format(accuracy))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy 93.8%\n"
     ]
    }
   ],
   "source": [
    "#print(whole_predict[700][700])\n",
    "whole_predict = np.asarray(whole_predict).astype(np.uint8)\n",
    "whole_predict_map = np.copy(whole_predict.astype(np.float32))\n",
    "whole_predict_map[whole_predict_map == 0] = -1\n",
    "whole_predict_map = ((whole_predict_map+1)*255/4).astype(np.uint8)\n",
    "whole_predict_map = cv2.applyColorMap(whole_predict_map, cv2.COLORMAP_JET)\n",
    "cv2.imwrite('AlexNetLike_final.png',whole_predict_map)\n",
    "\n",
    "truth_predict = Transect1_truth[crop_len+offstart:crop_len+offstart+num_lines, crop_len:Transect1_truth.shape[1]-crop_len]\n",
    "#print(truth_predict[700][700])\n",
    "accuracy = 100*(whole_predict == truth_predict).astype(np.float32).sum()/(whole_predict.shape[0]*whole_predict.shape[1])\n",
    "print('Final Accuracy %.1f%%' % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(weightfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
