{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WRITTEN BY ALAN LI\n",
    "# NASA AMES LABORATORY FOR ADVANCED SENSING (LAS)\n",
    "# Last edited: Aug 10, 2017\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Config the matplotlib backend as plotting in line in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image paths\n",
    "transect1 = '../Images/Transect 1 Hi-Res.tiff'\n",
    "transect1_truth = '../Images/Transect 1 Truth data.tif'\n",
    "\n",
    "# Load images\n",
    "img1 = cv2.imread(transect1,cv2.IMREAD_UNCHANGED)\n",
    "img1_truth = cv2.imread(transect1_truth,cv2.IMREAD_UNCHANGED)\n",
    "                \n",
    "# Remap truth data\n",
    "if transect1_truth == '../Images/Transect 1 Truth data.tif':\n",
    "    img1_truth[img1_truth == 16] = 0  # Sand\n",
    "    img1_truth[img1_truth == 160] = 1 # Branching\n",
    "    img1_truth[img1_truth == 198] = 2 # Mounding\n",
    "    img1_truth[img1_truth == 38] = 3 # Rock\n",
    "\n",
    "# NOTE: N_images = number of images in a set (e.g. training set, validation set, etc...)\n",
    "#       N_labels = # of classified pixels associated with an image (i.e. Might want to classify 4 central pixels of a NxN image)\n",
    "#       num_labels = # of labels (e.g. sand, rock, branching, mounding)\n",
    "#### Randomize set of data\n",
    "'''Input:\n",
    "    dataset: set of images, N_images x (nrow*ncol) x n_channels\n",
    "    labels: set of labels, N_images x N_labels x num_labels \n",
    "Output:\n",
    "    shuffled_dataset: set of randomized images, N_images x (nrow*ncol) x (n_channels-n_toremove)\n",
    "    shuffled_labels: set of randomized labels, N_images x N_labels x num_labels\n",
    "'''\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation,:,:]\n",
    "    return shuffled_dataset, shuffled_labels    \n",
    "    \n",
    "#### Reformat image into input vector style and randomize\n",
    "'''Input:\n",
    "    imgset: set of images, N_images x nrow x ncol x n_channels\n",
    "    toremove: index of channel dimension to remove\n",
    "    labels: set of labels, N_images x N_labels (N_labels > 1 when we want to classify a number of pixels per image) \n",
    "    num_labels: number of possible labels for the entire dataset (4 for morphology, 2 for coral cover)\n",
    "Output:\n",
    "    dataset: set of vectorized images, N_images x nrow x ncol x (n_channels-n_toremove)\n",
    "    labels: set of vectorized labels in the form of logits, N_images x N_labels x num_labels\n",
    "'''\n",
    "def reformat(imgset, toremove, labels, num_labels):    \n",
    "    imgsetcut = imgset\n",
    "    if toremove is not None:\n",
    "        imgsetcut = np.delete(imgset,toremove,-1) # Remove specific last dimension of array\n",
    "    labels = np.asarray([[(np.arange(num_labels) == labels[i,j]).astype(np.float32) for j in range(labels.shape[1])] for i in range(imgsetcut.shape[0])])\n",
    "    dataset, labels = randomize(imgsetcut, labels)\n",
    "    return dataset, labels\n",
    "\n",
    "#### Normalize Image\n",
    "'''Input:\n",
    "    dataset: set of vectorized images, N_images x nrow x ncol x n_channels\n",
    "    depth: pixel depth, float\n",
    "Output:\n",
    "    dataset_norm: set of vectorized normalized images, N_images x nrow x ncol x n_channels'''\n",
    "def rescale(dataset, depth):\n",
    "    dataset_norm = (dataset.astype(np.float32) - depth/2)/(depth/2)\n",
    "    return dataset_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Training, Validation and Test sets (could probably write a function for this later instead...)\n",
    "\n",
    "num_labels = 4\n",
    "image_size = 25 # side length of one sample image\n",
    "N_train = 20000 # number of training samples per class\n",
    "N_valid = 2500 # number of validation samples per class\n",
    "N_test = 2500 # number of test samples per class\n",
    "crop_len = int(np.floor(image_size/2))\n",
    "img1_truth_crop = img1_truth[crop_len:img1_truth.shape[0]-crop_len, crop_len:img1_truth.shape[1]-crop_len]\n",
    "\n",
    "# Randomly select points\n",
    "train_datasets = []\n",
    "train_labels = []\n",
    "valid_datasets = []\n",
    "valid_labels =[]\n",
    "test_datasets = []\n",
    "test_labels = []\n",
    "\n",
    "for k in range(num_labels):\n",
    "    [i,j] = np.where(img1_truth_crop == k)\n",
    "    # Training set\n",
    "    idx = np.asarray(random.sample(range(len(i)), N_train)).astype(int)\n",
    "    train_datasets.append([img1[i[idx[nn]]:i[idx[nn]]+image_size, j[idx[nn]]:j[idx[nn]]+image_size, :] for nn in range(len(idx))])\n",
    "    train_labels.append([img1_truth_crop[i[idx[nn]], j[idx[nn]]] for nn in range(len(idx))])\n",
    "    # Validation set\n",
    "    idx = np.asarray(random.sample(range(len(i)), N_valid)).astype(int)\n",
    "    valid_datasets.append([img1[i[idx[nn]]:i[idx[nn]]+image_size, j[idx[nn]]:j[idx[nn]]+image_size, :] for nn in range(len(idx))])\n",
    "    valid_labels.append([img1_truth_crop[i[idx[nn]], j[idx[nn]]] for nn in range(len(idx))])\n",
    "    # Test set\n",
    "    idx = np.asarray(random.sample(range(len(i)), N_test)).astype(int)\n",
    "    test_datasets.append([img1[i[idx[nn]]:i[idx[nn]]+image_size, j[idx[nn]]:j[idx[nn]]+image_size, :] for nn in range(len(idx))])\n",
    "    test_labels.append([img1_truth_crop[i[idx[nn]], j[idx[nn]]] for nn in range(len(idx))])\n",
    "\n",
    "# Some array handling and reshaping\n",
    "train_datasets = np.asarray(train_datasets) # train_datasets is in the format of num_labels x N_train x nrows x ncols x n_channels\n",
    "train_labels = np.asarray(train_labels) # train_labels is in the format of num_labels x N_train\n",
    "valid_datasets = np.asarray(valid_datasets)\n",
    "valid_labels = np.asarray(valid_labels) \n",
    "test_datasets = np.asarray(test_datasets) \n",
    "test_labels = np.asarray(test_labels) \n",
    "\n",
    "train_datasets = train_datasets.reshape(num_labels*N_train, image_size, image_size, img1.shape[-1]) # flatten first 2 dimensions of train_datasets\n",
    "train_labels = train_labels.reshape(num_labels*N_train,1) # flatten into vector\n",
    "valid_datasets = valid_datasets.reshape(num_labels*N_valid, image_size, image_size, img1.shape[-1]) \n",
    "valid_labels = valid_labels.reshape(num_labels*N_valid,1) \n",
    "test_datasets = test_datasets.reshape(num_labels*N_test, image_size, image_size, img1.shape[-1]) \n",
    "test_labels = test_labels.reshape(num_labels*N_test,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABrCAYAAABnlHmpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119074588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot sample images of different classes\n",
    "for i in range(num_labels):\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1, num_labels, i+1)\n",
    "    plt.imshow(cv2.cvtColor(train_datasets[i*N_train,:,:,0:3], cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset dimensions:  (80000, 25, 25, 3) Training labels dimensions:  (80000, 1, 4)\n",
      "Validation dataset dimensions:  (10000, 25, 25, 3) Validation labels dimensions:  (10000, 1, 4)\n",
      "Test dataset dimensions:  (10000, 25, 25, 3) Test labels dimensions:  (10000, 1, 4)\n",
      "Num_channels: 3 N_labels: 1\n"
     ]
    }
   ],
   "source": [
    "depth = 255.0\n",
    "\n",
    "# More array operations, randomize sets, and normalize from -1 to 1\n",
    "train_dataset, train_labelset = reformat(train_datasets, 3, train_labels, num_labels) # Reformat shape into batch arrays\n",
    "valid_dataset, valid_labelset = reformat(valid_datasets, 3, valid_labels, num_labels)\n",
    "test_dataset, test_labelset = reformat(test_datasets, 3, test_labels, num_labels)\n",
    "print(\"Training dataset dimensions: \", train_dataset.shape, \"Training labels dimensions: \", train_labelset.shape)\n",
    "print(\"Validation dataset dimensions: \", valid_dataset.shape, \"Validation labels dimensions: \", valid_labelset.shape)\n",
    "print(\"Test dataset dimensions: \", test_dataset.shape, \"Test labels dimensions: \", test_labelset.shape)\n",
    "\n",
    "train_dataset = rescale(train_dataset,depth)\n",
    "valid_dataset = rescale(valid_dataset,depth)\n",
    "test_dataset = rescale(test_dataset,depth)\n",
    "\n",
    "num_channels = train_dataset.shape[-1]\n",
    "N_labels = train_labelset.shape[1]\n",
    "print(\"Num_channels:\", num_channels, \"N_labels:\", N_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow Implementation\n",
    "batch_size = 32\n",
    "patch1_size = 5\n",
    "stride1 = 2\n",
    "patch2_size = 5\n",
    "stride2 = 1\n",
    "depth1 = 64\n",
    "depth2 = 128\n",
    "depth3 = 256\n",
    "depth4 = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Datasets\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size*image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape(batch_size,N_labels,num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Size of arrays\n",
    "    layer1size = (image_size-patch1_size)//stride1 + 1\n",
    "    layer2size = (conv1size-patch2_size)//stride2 + 1\n",
    "    \n",
    "    # Variables\n",
    "    C1_weights = tf.Variable(tf.truncated_normal([patch1_size, patch1_size, num_channels, depth1], stddev=0.1))\n",
    "    C1_biases = tf.Variable(tf.zeros([depth1]))\n",
    "    C2_weights = tf.Variable(tf.truncated_normal([patch2_size, patch2_size, depth1, depth2], stddev=0.1))\n",
    "    C2_biases = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "    F3_weights = tf.Variable(tf.truncated_normal([layer2size*layer2size*depth2, depth3], stddev=0.1))\n",
    "    F3_biases = tf.Variable(tf.constant(1.0, shape=[depth3]))\n",
    "    F4_weights = tf.Variable(tf.truncated_normal([depth3, depth4], stddev=0.1))\n",
    "    F4_biases = tf.Variable(tf.constant(1.0, shape=[depth4]))\n",
    "    F5_weights = tf.Variable(tf.truncated_normal([depth4, N_labels, num_labels], stddev=0.1))\n",
    "    F5_biases = tf.Variable(tf.constant(1.0, shape=[N_labels,num_labels]))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
